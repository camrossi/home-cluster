# https://github.com/traefik/traefik-helm-chart/issues/791#issuecomment-1426940654
instanceLabelOverride: "traefik"
service:
  enabled: true
  single: true
  type: LoadBalancer
  annotations: 
    io.cilium/lb-ipam-ips: 192.168.0.226
  labels: 
    type: l2
# This is needed to Argo dosen't get stuck on Sync ingress forever
# https://github.com/argoproj/argo-cd/issues/968#issuecomment-451082913
providers:
  kubernetesIngress:
    publishedService:
      enabled: true
    kubernetesCRD:
      allowCrossNamespace: true
api:
  dashboard: true
metrics:
  prometheus:
    service:
      enabled: true
    serviceMonitor:
      additionalLabels:
        # This is needed to make the ServiceMonitor being picked up by prometheus ServiceMonitorSelector
        release: kube-prometheus-stack
      metricRelabelings:
        - sourceLabels: [__name__]
          separator: ;
          regex: ^fluentd_output_status_buffer_(oldest|newest)_.+
          replacement: $1
          action: drop
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          separator: ;
          regex: ^(.*)$
          targetLabel: nodename
          replacement: $1
          action: replace
      jobLabel: traefik
      interval: 30s
      honorLabels: true
    prometheusRule:
      additionalLabels:
        # This is needed to make the ServiceMonitor being picked up by prometheus ServiceMonitorSelector
        release: kube-prometheus-stack
      rules:
        - alert: TraefikDown
          expr: up{job="traefik"} == 0
          for: 5m
          labels:
            context: traefik
            severity: warning
          annotations:
            summary: "Traefik Down"
            description: "{{ $labels.pod }} on {{ $labels.nodename }} is down"
ingressRoute:
  dashboard:
    entryPoints: ["web"]